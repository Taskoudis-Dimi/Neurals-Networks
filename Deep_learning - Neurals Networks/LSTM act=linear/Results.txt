Training iter #1500:   Batch Loss = 2.798674, Accuracy = 0.23933333158493042
PERFORMANCE ON TEST SET: Batch Loss = 2.323054552078247, Accuracy = 0.3664743900299072
Training iter #30000:   Batch Loss = 1.278338, Accuracy = 0.7106666564941406
PERFORMANCE ON TEST SET: Batch Loss = 1.4086029529571533, Accuracy = 0.6613505482673645
Training iter #60000:   Batch Loss = 1.183184, Accuracy = 0.7793333530426025
PERFORMANCE ON TEST SET: Batch Loss = 1.2978301048278809, Accuracy = 0.7390566468238831
Training iter #90000:   Batch Loss = 0.997279, Accuracy = 0.8493333458900452
PERFORMANCE ON TEST SET: Batch Loss = 1.3543429374694824, Accuracy = 0.7587376832962036
Training iter #120000:   Batch Loss = 0.903578, Accuracy = 0.8786666393280029
PERFORMANCE ON TEST SET: Batch Loss = 1.298932433128357, Accuracy = 0.7974210977554321
Training iter #150000:   Batch Loss = 0.824781, Accuracy = 0.9153333306312561
PERFORMANCE ON TEST SET: Batch Loss = 1.2266745567321777, Accuracy = 0.817441463470459
Training iter #180000:   Batch Loss = 0.888718, Accuracy = 0.8960000276565552
PERFORMANCE ON TEST SET: Batch Loss = 1.2671301364898682, Accuracy = 0.7872412800788879
Training iter #210000:   Batch Loss = 0.820725, Accuracy = 0.8946666717529297
PERFORMANCE ON TEST SET: Batch Loss = 1.0655421018600464, Accuracy = 0.8591788411140442
Training iter #240000:   Batch Loss = 0.632691, Accuracy = 0.9679999947547913
PERFORMANCE ON TEST SET: Batch Loss = 1.0506141185760498, Accuracy = 0.8635901212692261
Training iter #270000:   Batch Loss = 0.638672, Accuracy = 0.9559999704360962
PERFORMANCE ON TEST SET: Batch Loss = 1.076194167137146, Accuracy = 0.8405157923698425
Training iter #300000:   Batch Loss = 0.770059, Accuracy = 0.9359999895095825
PERFORMANCE ON TEST SET: Batch Loss = 0.9445704221725464, Accuracy = 0.8775025606155396
Training iter #330000:   Batch Loss = 0.666790, Accuracy = 0.9646666646003723
PERFORMANCE ON TEST SET: Batch Loss = 0.9772448539733887, Accuracy = 0.8652867078781128
Training iter #360000:   Batch Loss = 0.648083, Accuracy = 0.9486666917800903
PERFORMANCE ON TEST SET: Batch Loss = 0.9655735492706299, Accuracy = 0.8808958530426025
Training iter #390000:   Batch Loss = 0.638790, Accuracy = 0.9440000057220459
PERFORMANCE ON TEST SET: Batch Loss = 0.9548890590667725, Accuracy = 0.8819137811660767
Training iter #420000:   Batch Loss = 0.575986, Accuracy = 0.95333331823349
PERFORMANCE ON TEST SET: Batch Loss = 0.962988018989563, Accuracy = 0.8703766465187073
Training iter #450000:   Batch Loss = 0.579741, Accuracy = 0.9413333535194397
PERFORMANCE ON TEST SET: Batch Loss = 0.995038628578186, Accuracy = 0.8669833540916443
Training iter #480000:   Batch Loss = 0.594734, Accuracy = 0.9240000247955322
PERFORMANCE ON TEST SET: Batch Loss = 0.9309780597686768, Accuracy = 0.8829317688941956
Training iter #510000:   Batch Loss = 0.577376, Accuracy = 0.9473333358764648
PERFORMANCE ON TEST SET: Batch Loss = 0.9191732406616211, Accuracy = 0.879199206829071
Training iter #540000:   Batch Loss = 0.570666, Accuracy = 0.9306666851043701
PERFORMANCE ON TEST SET: Batch Loss = 1.0260452032089233, Accuracy = 0.8493382930755615
Training iter #570000:   Batch Loss = 0.537404, Accuracy = 0.9340000152587891
PERFORMANCE ON TEST SET: Batch Loss = 1.0346183776855469, Accuracy = 0.8411944508552551
Training iter #600000:   Batch Loss = 0.515189, Accuracy = 0.9539999961853027
PERFORMANCE ON TEST SET: Batch Loss = 0.9619199633598328, Accuracy = 0.8730912804603577
Training iter #630000:   Batch Loss = 0.469791, Accuracy = 0.9739999771118164
PERFORMANCE ON TEST SET: Batch Loss = 1.0615098476409912, Accuracy = 0.851034939289093
Training iter #660000:   Batch Loss = 0.465909, Accuracy = 0.9660000205039978
PERFORMANCE ON TEST SET: Batch Loss = 0.9964312314987183, Accuracy = 0.8574821949005127
Training iter #690000:   Batch Loss = 0.494921, Accuracy = 0.9520000219345093
PERFORMANCE ON TEST SET: Batch Loss = 0.8087756633758545, Accuracy = 0.8825924396514893
Training iter #720000:   Batch Loss = 0.508530, Accuracy = 0.9520000219345093
PERFORMANCE ON TEST SET: Batch Loss = 0.8505458831787109, Accuracy = 0.8937903046607971
Training iter #750000:   Batch Loss = 0.569678, Accuracy = 0.9226666688919067
PERFORMANCE ON TEST SET: Batch Loss = 0.90152907371521, Accuracy = 0.8931116461753845
Training iter #780000:   Batch Loss = 0.462591, Accuracy = 0.9613333344459534
PERFORMANCE ON TEST SET: Batch Loss = 0.8790637254714966, Accuracy = 0.8795385360717773
Training iter #810000:   Batch Loss = 0.439137, Accuracy = 0.9620000123977661
PERFORMANCE ON TEST SET: Batch Loss = 0.854637861251831, Accuracy = 0.8924329876899719
Training iter #840000:   Batch Loss = 0.450871, Accuracy = 0.9359999895095825
PERFORMANCE ON TEST SET: Batch Loss = 0.8762409687042236, Accuracy = 0.8802171945571899
Training iter #870000:   Batch Loss = 0.426321, Accuracy = 0.9513333439826965
PERFORMANCE ON TEST SET: Batch Loss = 0.8750866651535034, Accuracy = 0.8883610367774963
Training iter #900000:   Batch Loss = 0.394922, Accuracy = 0.9713333249092102
PERFORMANCE ON TEST SET: Batch Loss = 0.8675180673599243, Accuracy = 0.8825924396514893
Training iter #930000:   Batch Loss = 0.437371, Accuracy = 0.9340000152587891
PERFORMANCE ON TEST SET: Batch Loss = 0.8194034099578857, Accuracy = 0.8870037198066711
Training iter #960000:   Batch Loss = 0.412692, Accuracy = 0.9466666579246521
PERFORMANCE ON TEST SET: Batch Loss = 0.8293891549110413, Accuracy = 0.8839497566223145
Training iter #990000:   Batch Loss = 0.365262, Accuracy = 0.9853333234786987
PERFORMANCE ON TEST SET: Batch Loss = 0.8341495990753174, Accuracy = 0.873430609703064
Training iter #1020000:   Batch Loss = 0.372240, Accuracy = 0.9826666712760925
PERFORMANCE ON TEST SET: Batch Loss = 0.7667999267578125, Accuracy = 0.8897183537483215
Training iter #1050000:   Batch Loss = 0.356387, Accuracy = 0.9926666617393494
PERFORMANCE ON TEST SET: Batch Loss = 0.8242075443267822, Accuracy = 0.8873430490493774
Training iter #1080000:   Batch Loss = 0.375586, Accuracy = 0.9739999771118164
PERFORMANCE ON TEST SET: Batch Loss = 0.8616188168525696, Accuracy = 0.8737699389457703
Training iter #1110000:   Batch Loss = 0.409045, Accuracy = 0.9446666836738586
PERFORMANCE ON TEST SET: Batch Loss = 0.8173028230667114, Accuracy = 0.8812351822853088
Training iter #1140000:   Batch Loss = 0.396489, Accuracy = 0.9513333439826965
PERFORMANCE ON TEST SET: Batch Loss = 0.8014274835586548, Accuracy = 0.8859857320785522
Training iter #1170000:   Batch Loss = 0.342212, Accuracy = 0.9580000042915344
PERFORMANCE ON TEST SET: Batch Loss = 0.793682873249054, Accuracy = 0.884628415107727
Training iter #1200000:   Batch Loss = 0.343970, Accuracy = 0.9653333425521851
PERFORMANCE ON TEST SET: Batch Loss = 0.9150965809822083, Accuracy = 0.8513742685317993
Training iter #1230000:   Batch Loss = 0.438735, Accuracy = 0.9193333387374878
PERFORMANCE ON TEST SET: Batch Loss = 1.061264991760254, Accuracy = 0.7790973782539368
Training iter #1260000:   Batch Loss = 0.509991, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 0.632088303565979, Accuracy = 0.8788598775863647
Training iter #1290000:   Batch Loss = 0.455414, Accuracy = 0.8933333158493042
PERFORMANCE ON TEST SET: Batch Loss = 0.7278671264648438, Accuracy = 0.8744485974311829
Training iter #1320000:   Batch Loss = 0.409483, Accuracy = 0.9286666512489319
PERFORMANCE ON TEST SET: Batch Loss = 0.6651365756988525, Accuracy = 0.9077027440071106
Training iter #1350000:   Batch Loss = 0.341594, Accuracy = 0.9679999947547913
PERFORMANCE ON TEST SET: Batch Loss = 0.5763449668884277, Accuracy = 0.9141499996185303
Training iter #1380000:   Batch Loss = 0.314171, Accuracy = 0.9860000014305115
PERFORMANCE ON TEST SET: Batch Loss = 0.5985342264175415, Accuracy = 0.9144893288612366
Training iter #1410000:   Batch Loss = 0.299438, Accuracy = 0.9853333234786987
PERFORMANCE ON TEST SET: Batch Loss = 0.6245712637901306, Accuracy = 0.910417377948761
Training iter #1440000:   Batch Loss = 0.317283, Accuracy = 0.9819999933242798
PERFORMANCE ON TEST SET: Batch Loss = 0.614268958568573, Accuracy = 0.9110960364341736
Training iter #1470000:   Batch Loss = 0.344936, Accuracy = 0.9480000138282776
PERFORMANCE ON TEST SET: Batch Loss = 0.6198186278343201, Accuracy = 0.9124533534049988
Training iter #1500000:   Batch Loss = 0.353318, Accuracy = 0.9513333439826965
PERFORMANCE ON TEST SET: Batch Loss = 0.6602464318275452, Accuracy = 0.8985409140586853
Training iter #1530000:   Batch Loss = 0.286032, Accuracy = 0.9733333587646484
PERFORMANCE ON TEST SET: Batch Loss = 0.6688177585601807, Accuracy = 0.8815745115280151
Training iter #1560000:   Batch Loss = 0.301569, Accuracy = 0.9633333086967468
PERFORMANCE ON TEST SET: Batch Loss = 0.6109853386878967, Accuracy = 0.9056667685508728
Training iter #1590000:   Batch Loss = 0.320352, Accuracy = 0.9419999718666077
PERFORMANCE ON TEST SET: Batch Loss = 0.6465550661087036, Accuracy = 0.9032914638519287
Training iter #1620000:   Batch Loss = 0.289519, Accuracy = 0.9739999771118164
PERFORMANCE ON TEST SET: Batch Loss = 0.6453253626823425, Accuracy = 0.9043094515800476
Training iter #1650000:   Batch Loss = 0.396221, Accuracy = 0.921999990940094
PERFORMANCE ON TEST SET: Batch Loss = 0.6867350935935974, Accuracy = 0.8690193295478821
Training iter #1680000:   Batch Loss = 0.390848, Accuracy = 0.9066666960716248
PERFORMANCE ON TEST SET: Batch Loss = 0.636642575263977, Accuracy = 0.898201584815979
Training iter #1710000:   Batch Loss = 0.368276, Accuracy = 0.9346666932106018
PERFORMANCE ON TEST SET: Batch Loss = 0.6274481415748596, Accuracy = 0.8998982310295105
Training iter #1740000:   Batch Loss = 0.269103, Accuracy = 0.9786666631698608
PERFORMANCE ON TEST SET: Batch Loss = 0.6155363321304321, Accuracy = 0.8968442678451538
Training iter #1770000:   Batch Loss = 0.266086, Accuracy = 0.9879999756813049
PERFORMANCE ON TEST SET: Batch Loss = 0.6158686280250549, Accuracy = 0.8998982310295105
Training iter #1800000:   Batch Loss = 0.258764, Accuracy = 0.984000027179718
PERFORMANCE ON TEST SET: Batch Loss = 0.612120509147644, Accuracy = 0.9049881100654602
Training iter #1830000:   Batch Loss = 0.318660, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 0.6907438635826111, Accuracy = 0.8805565237998962
Training iter #1860000:   Batch Loss = 0.367723, Accuracy = 0.9293333292007446
PERFORMANCE ON TEST SET: Batch Loss = 0.6908997893333435, Accuracy = 0.8805565237998962
Training iter #1890000:   Batch Loss = 0.312049, Accuracy = 0.9440000057220459
PERFORMANCE ON TEST SET: Batch Loss = 0.6275627017021179, Accuracy = 0.9009161591529846
Training iter #1920000:   Batch Loss = 0.268841, Accuracy = 0.9573333263397217
PERFORMANCE ON TEST SET: Batch Loss = 0.598931074142456, Accuracy = 0.8965049386024475
Training iter #1950000:   Batch Loss = 0.276390, Accuracy = 0.9606666564941406
PERFORMANCE ON TEST SET: Batch Loss = 0.620479166507721, Accuracy = 0.8988802433013916
Training iter #1980000:   Batch Loss = 0.285665, Accuracy = 0.940666675567627
PERFORMANCE ON TEST SET: Batch Loss = 0.6310291290283203, Accuracy = 0.8988802433013916
Training iter #2010000:   Batch Loss = 0.250581, Accuracy = 0.9666666388511658
PERFORMANCE ON TEST SET: Batch Loss = 0.6571090221405029, Accuracy = 0.9049881100654602
Training iter #2040000:   Batch Loss = 0.295816, Accuracy = 0.9273333549499512
PERFORMANCE ON TEST SET: Batch Loss = 0.674543559551239, Accuracy = 0.8866643905639648
Training iter #2070000:   Batch Loss = 0.356026, Accuracy = 0.9206666946411133
PERFORMANCE ON TEST SET: Batch Loss = 0.6465664505958557, Accuracy = 0.885646402835846
Training iter #2100000:   Batch Loss = 0.239310, Accuracy = 0.9879999756813049
PERFORMANCE ON TEST SET: Batch Loss = 0.5791109800338745, Accuracy = 0.9056667685508728
Training iter #2130000:   Batch Loss = 0.241887, Accuracy = 0.9760000109672546
PERFORMANCE ON TEST SET: Batch Loss = 0.6237615942955017, Accuracy = 0.9043094515800476
Training iter #2160000:   Batch Loss = 0.252028, Accuracy = 0.9620000123977661
PERFORMANCE ON TEST SET: Batch Loss = 0.6378798484802246, Accuracy = 0.884628415107727
Training iter #2190000:   Batch Loss = 0.257019, Accuracy = 0.9766666889190674
PERFORMANCE ON TEST SET: Batch Loss = 0.740001380443573, Accuracy = 0.8581608533859253
Optimization Finished!
FINAL RESULT: Batch Loss = 0.7530260682106018, Accuracy = 0.8557855486869812




Testing Accuracy: 85.57855486869812%

Precision: 86.84138831921221%
Recall: 85.57855446216492%
f1_score: 85.67999057359924%

Confusion Matrix:
[[411   0  85   0   0   0]
 [ 26 364  80   0   1   0]
 [  4   1 415   0   0   0]
 [  2  24   0 416  49   0]
 [  2   3   2 119 406   0]
 [  0  27   0   0   0 510]]

Confusion matrix (normalised to % of total test data):
[[13.946385    0.          2.884289    0.          0.          0.        ]
 [ 0.88225317 12.351544    2.714625    0.          0.03393281  0.        ]
 [ 0.13573125  0.03393281 14.082117    0.          0.          0.        ]
 [ 0.06786563  0.8143875   0.         14.116051    1.6627079   0.        ]
 [ 0.06786563  0.10179844  0.06786563  4.0380044  13.776723    0.        ]
 [ 0.          0.916186    0.          0.          0.         17.305735  ]]
Note: training and testing data is not equally distributed amongst classes, 
so it is normal that more than a 6th of the data is correctly classifier in the last category.