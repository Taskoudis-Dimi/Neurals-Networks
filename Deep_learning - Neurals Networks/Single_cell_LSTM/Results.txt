raining iter #1500:   Batch Loss = 3.932308, Accuracy = 0.14800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 3.169116973876953, Accuracy = 0.15134035050868988
Training iter #30000:   Batch Loss = 2.401728, Accuracy = 0.17800000309944153
PERFORMANCE ON TEST SET: Batch Loss = 2.3928794860839844, Accuracy = 0.17543263733386993
Training iter #60000:   Batch Loss = 2.354876, Accuracy = 0.16200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 2.341963768005371, Accuracy = 0.17000339925289154
Training iter #90000:   Batch Loss = 2.331858, Accuracy = 0.16733333468437195
PERFORMANCE ON TEST SET: Batch Loss = 2.31950306892395, Accuracy = 0.17611129581928253
Training iter #120000:   Batch Loss = 2.301577, Accuracy = 0.16733333468437195
PERFORMANCE ON TEST SET: Batch Loss = 2.2986316680908203, Accuracy = 0.17407533526420593
Training iter #150000:   Batch Loss = 2.272160, Accuracy = 0.18400000035762787
PERFORMANCE ON TEST SET: Batch Loss = 2.2817461490631104, Accuracy = 0.17034272849559784
Training iter #180000:   Batch Loss = 2.258245, Accuracy = 0.18333333730697632
PERFORMANCE ON TEST SET: Batch Loss = 2.263822555541992, Accuracy = 0.17543263733386993
Training iter #210000:   Batch Loss = 2.227689, Accuracy = 0.20733332633972168
PERFORMANCE ON TEST SET: Batch Loss = 2.244004011154175, Accuracy = 0.17645062506198883
Training iter #240000:   Batch Loss = 2.212516, Accuracy = 0.1899999976158142
PERFORMANCE ON TEST SET: Batch Loss = 2.225485324859619, Accuracy = 0.1879877895116806
Training iter #270000:   Batch Loss = 2.205551, Accuracy = 0.18666666746139526
PERFORMANCE ON TEST SET: Batch Loss = 2.209947109222412, Accuracy = 0.1917203962802887
Training iter #300000:   Batch Loss = 2.189634, Accuracy = 0.19866666197776794
PERFORMANCE ON TEST SET: Batch Loss = 2.1941230297088623, Accuracy = 0.18459449708461761
Training iter #330000:   Batch Loss = 2.176246, Accuracy = 0.1860000044107437
PERFORMANCE ON TEST SET: Batch Loss = 2.180825710296631, Accuracy = 0.18018323183059692
Training iter #360000:   Batch Loss = 2.154225, Accuracy = 0.20800000429153442
PERFORMANCE ON TEST SET: Batch Loss = 2.1674108505249023, Accuracy = 0.17271801829338074
Training iter #390000:   Batch Loss = 2.157118, Accuracy = 0.17599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 2.153393507003784, Accuracy = 0.17984390258789062
Training iter #420000:   Batch Loss = 2.140088, Accuracy = 0.17533333599567413
PERFORMANCE ON TEST SET: Batch Loss = 2.1392648220062256, Accuracy = 0.18154054880142212
Training iter #450000:   Batch Loss = 2.128782, Accuracy = 0.1733333319425583
PERFORMANCE ON TEST SET: Batch Loss = 2.1256940364837646, Accuracy = 0.1869698017835617
Training iter #480000:   Batch Loss = 2.116620, Accuracy = 0.19466666877269745
PERFORMANCE ON TEST SET: Batch Loss = 2.1139473915100098, Accuracy = 0.1859518140554428
Training iter #510000:   Batch Loss = 2.105990, Accuracy = 0.17866666615009308
PERFORMANCE ON TEST SET: Batch Loss = 2.1001009941101074, Accuracy = 0.17814727127552032
Training iter #540000:   Batch Loss = 2.083077, Accuracy = 0.218666672706604
PERFORMANCE ON TEST SET: Batch Loss = 2.0881428718566895, Accuracy = 0.1856124848127365
Training iter #570000:   Batch Loss = 2.063987, Accuracy = 0.20533333718776703
PERFORMANCE ON TEST SET: Batch Loss = 2.07783842086792, Accuracy = 0.18086189031600952
Training iter #600000:   Batch Loss = 2.051326, Accuracy = 0.19066666066646576
PERFORMANCE ON TEST SET: Batch Loss = 2.067406415939331, Accuracy = 0.1856124848127365
Training iter #630000:   Batch Loss = 2.045192, Accuracy = 0.2006666660308838
PERFORMANCE ON TEST SET: Batch Loss = 2.0581300258636475, Accuracy = 0.18187987804412842
Training iter #660000:   Batch Loss = 2.040659, Accuracy = 0.1926666647195816
PERFORMANCE ON TEST SET: Batch Loss = 2.047010898590088, Accuracy = 0.18255853652954102
Training iter #690000:   Batch Loss = 2.033909, Accuracy = 0.19466666877269745
PERFORMANCE ON TEST SET: Batch Loss = 2.0379598140716553, Accuracy = 0.18018323183059692
Training iter #720000:   Batch Loss = 2.019485, Accuracy = 0.19466666877269745
PERFORMANCE ON TEST SET: Batch Loss = 2.029179096221924, Accuracy = 0.188666433095932
Training iter #750000:   Batch Loss = 2.013884, Accuracy = 0.195333331823349
PERFORMANCE ON TEST SET: Batch Loss = 2.0194973945617676, Accuracy = 0.1927383840084076
Training iter #780000:   Batch Loss = 2.019458, Accuracy = 0.18933333456516266
PERFORMANCE ON TEST SET: Batch Loss = 2.012314558029175, Accuracy = 0.1907024085521698
Training iter #810000:   Batch Loss = 2.011974, Accuracy = 0.1613333374261856
PERFORMANCE ON TEST SET: Batch Loss = 2.005263328552246, Accuracy = 0.1896844208240509
Training iter #840000:   Batch Loss = 1.999453, Accuracy = 0.195333331823349
PERFORMANCE ON TEST SET: Batch Loss = 1.996976613998413, Accuracy = 0.1849338263273239
Training iter #870000:   Batch Loss = 1.989901, Accuracy = 0.1733333319425583
PERFORMANCE ON TEST SET: Batch Loss = 1.989452838897705, Accuracy = 0.1883271187543869
Training iter #900000:   Batch Loss = 1.985871, Accuracy = 0.18266665935516357
PERFORMANCE ON TEST SET: Batch Loss = 1.9815542697906494, Accuracy = 0.1839158535003662
Training iter #930000:   Batch Loss = 1.972270, Accuracy = 0.18199999630451202
PERFORMANCE ON TEST SET: Batch Loss = 1.9743916988372803, Accuracy = 0.1852731555700302
Training iter #960000:   Batch Loss = 1.953110, Accuracy = 0.20333333313465118
PERFORMANCE ON TEST SET: Batch Loss = 1.9687891006469727, Accuracy = 0.1832371950149536
Training iter #990000:   Batch Loss = 1.942576, Accuracy = 0.20600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 1.9631868600845337, Accuracy = 0.18086189031600952
Training iter #1020000:   Batch Loss = 1.944255, Accuracy = 0.19466666877269745
PERFORMANCE ON TEST SET: Batch Loss = 1.9572821855545044, Accuracy = 0.188666433095932
Training iter #1050000:   Batch Loss = 1.942147, Accuracy = 0.20266667008399963
PERFORMANCE ON TEST SET: Batch Loss = 1.9522393941879272, Accuracy = 0.17916525900363922
Training iter #1080000:   Batch Loss = 1.938731, Accuracy = 0.18733333051204681
PERFORMANCE ON TEST SET: Batch Loss = 1.9466047286987305, Accuracy = 0.18425516784191132
Training iter #1110000:   Batch Loss = 1.931405, Accuracy = 0.1993333399295807
PERFORMANCE ON TEST SET: Batch Loss = 1.94096040725708, Accuracy = 0.17339667677879333
Training iter #1140000:   Batch Loss = 1.933734, Accuracy = 0.1899999976158142
PERFORMANCE ON TEST SET: Batch Loss = 1.9364205598831177, Accuracy = 0.17441466450691223
Training iter #1170000:   Batch Loss = 1.940965, Accuracy = 0.164000004529953
PERFORMANCE ON TEST SET: Batch Loss = 1.9311258792877197, Accuracy = 0.1876484602689743
Training iter #1200000:   Batch Loss = 1.938801, Accuracy = 0.18133333325386047
PERFORMANCE ON TEST SET: Batch Loss = 1.9263806343078613, Accuracy = 0.18154054880142212
Training iter #1230000:   Batch Loss = 1.927611, Accuracy = 0.17466667294502258
PERFORMANCE ON TEST SET: Batch Loss = 1.9210405349731445, Accuracy = 0.17814727127552032
Training iter #1260000:   Batch Loss = 1.913938, Accuracy = 0.1940000057220459
PERFORMANCE ON TEST SET: Batch Loss = 1.9163973331451416, Accuracy = 0.187309131026268
Training iter #1290000:   Batch Loss = 1.905752, Accuracy = 0.18199999630451202
PERFORMANCE ON TEST SET: Batch Loss = 1.9128237962722778, Accuracy = 0.18120121955871582
Training iter #1320000:   Batch Loss = 1.899604, Accuracy = 0.1926666647195816
PERFORMANCE ON TEST SET: Batch Loss = 1.9085066318511963, Accuracy = 0.1896844208240509
Training iter #1350000:   Batch Loss = 1.888477, Accuracy = 0.19333332777023315
PERFORMANCE ON TEST SET: Batch Loss = 1.9045218229293823, Accuracy = 0.18187987804412842
Training iter #1380000:   Batch Loss = 1.883204, Accuracy = 0.1940000057220459
PERFORMANCE ON TEST SET: Batch Loss = 1.9023220539093018, Accuracy = 0.18052256107330322
Training iter #1410000:   Batch Loss = 1.887442, Accuracy = 0.18533332645893097
PERFORMANCE ON TEST SET: Batch Loss = 1.8980863094329834, Accuracy = 0.17916525900363922
Training iter #1440000:   Batch Loss = 1.881941, Accuracy = 0.1913333386182785
PERFORMANCE ON TEST SET: Batch Loss = 1.8954185247421265, Accuracy = 0.18221920728683472
Training iter #1470000:   Batch Loss = 1.880239, Accuracy = 0.1926666647195816
PERFORMANCE ON TEST SET: Batch Loss = 1.8922829627990723, Accuracy = 0.18187987804412842
Training iter #1500000:   Batch Loss = 1.882488, Accuracy = 0.1860000044107437
PERFORMANCE ON TEST SET: Batch Loss = 1.8881467580795288, Accuracy = 0.1835765242576599
Training iter #1530000:   Batch Loss = 1.883235, Accuracy = 0.19200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 1.8853588104248047, Accuracy = 0.18425516784191132
Training iter #1560000:   Batch Loss = 1.886394, Accuracy = 0.1706666648387909
PERFORMANCE ON TEST SET: Batch Loss = 1.8817849159240723, Accuracy = 0.1862911432981491
Training iter #1590000:   Batch Loss = 1.883447, Accuracy = 0.18533332645893097
PERFORMANCE ON TEST SET: Batch Loss = 1.87874436378479, Accuracy = 0.17848660051822662
Training iter #1620000:   Batch Loss = 1.883286, Accuracy = 0.1733333319425583
PERFORMANCE ON TEST SET: Batch Loss = 1.876630187034607, Accuracy = 0.1893450915813446
Training iter #1650000:   Batch Loss = 1.866439, Accuracy = 0.17666666209697723
PERFORMANCE ON TEST SET: Batch Loss = 1.8738652467727661, Accuracy = 0.1849338263273239
Training iter #1680000:   Batch Loss = 1.858603, Accuracy = 0.1979999989271164
PERFORMANCE ON TEST SET: Batch Loss = 1.8716225624084473, Accuracy = 0.17441466450691223
Training iter #1710000:   Batch Loss = 1.851182, Accuracy = 0.21199999749660492
PERFORMANCE ON TEST SET: Batch Loss = 1.8688148260116577, Accuracy = 0.17712928354740143
Training iter #1740000:   Batch Loss = 1.847227, Accuracy = 0.21133333444595337
PERFORMANCE ON TEST SET: Batch Loss = 1.8655874729156494, Accuracy = 0.17475399374961853
Training iter #1770000:   Batch Loss = 1.854014, Accuracy = 0.18733333051204681
PERFORMANCE ON TEST SET: Batch Loss = 1.8636358976364136, Accuracy = 0.1856124848127365
Training iter #1800000:   Batch Loss = 1.855724, Accuracy = 0.17866666615009308
PERFORMANCE ON TEST SET: Batch Loss = 1.861664056777954, Accuracy = 0.17441466450691223
Training iter #1830000:   Batch Loss = 1.850864, Accuracy = 0.20200000703334808
PERFORMANCE ON TEST SET: Batch Loss = 1.8606034517288208, Accuracy = 0.17882592976093292
Training iter #1860000:   Batch Loss = 1.849778, Accuracy = 0.19733333587646484
PERFORMANCE ON TEST SET: Batch Loss = 1.8586581945419312, Accuracy = 0.17543263733386993
Training iter #1890000:   Batch Loss = 1.862115, Accuracy = 0.17599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 1.8560093641281128, Accuracy = 0.17543263733386993
Training iter #1920000:   Batch Loss = 1.857569, Accuracy = 0.17399999499320984
PERFORMANCE ON TEST SET: Batch Loss = 1.8535479307174683, Accuracy = 0.18154054880142212
Training iter #1950000:   Batch Loss = 1.854464, Accuracy = 0.17399999499320984
PERFORMANCE ON TEST SET: Batch Loss = 1.8512696027755737, Accuracy = 0.18018323183059692
Training iter #1980000:   Batch Loss = 1.849794, Accuracy = 0.18466666340827942
PERFORMANCE ON TEST SET: Batch Loss = 1.8497183322906494, Accuracy = 0.17746861279010773
Training iter #2010000:   Batch Loss = 1.851649, Accuracy = 0.17399999499320984
PERFORMANCE ON TEST SET: Batch Loss = 1.8482904434204102, Accuracy = 0.18018323183059692
Training iter #2040000:   Batch Loss = 1.843690, Accuracy = 0.1913333386182785
PERFORMANCE ON TEST SET: Batch Loss = 1.8466132879257202, Accuracy = 0.18289786577224731
Training iter #2070000:   Batch Loss = 1.825837, Accuracy = 0.22200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 1.844927191734314, Accuracy = 0.17950458824634552
Training iter #2100000:   Batch Loss = 1.828020, Accuracy = 0.20800000429153442
PERFORMANCE ON TEST SET: Batch Loss = 1.8433030843734741, Accuracy = 0.18221920728683472
Training iter #2130000:   Batch Loss = 1.830198, Accuracy = 0.2006666660308838
PERFORMANCE ON TEST SET: Batch Loss = 1.8412731885910034, Accuracy = 0.1876484602689743
Training iter #2160000:   Batch Loss = 1.832927, Accuracy = 0.19599999487400055
PERFORMANCE ON TEST SET: Batch Loss = 1.839938759803772, Accuracy = 0.18187987804412842
Training iter #2190000:   Batch Loss = 1.833403, Accuracy = 0.20466665923595428
PERFORMANCE ON TEST SET: Batch Loss = 1.8388783931732178, Accuracy = 0.17339667677879333
Optimization Finished!
FINAL RESULT: Batch Loss = 1.8385487794876099, Accuracy = 0.17984390258789062


Testing Accuracy: 17.984390258789062%

Precision: 6.129556815050724%
Recall: 17.98439090600611%
f1_score: 6.588070764017198%

Confusion Matrix:
[[  0   0   0   0  35 461]
 [  0   0   0   0  24 447]
 [  0   0   0   0  17 403]
 [  0   0   0   0  22 469]
 [  0   0   0   0  21 511]
 [  0   0   0   0  39 498]]

Confusion matrix (normalised to % of total test data):
[[ 0.         0.         0.         0.         1.1876484 15.643027 ]
 [ 0.         0.         0.         0.         0.8143875 15.167968 ]
 [ 0.         0.         0.         0.         0.5768578 13.674924 ]
 [ 0.         0.         0.         0.         0.7465219 15.91449  ]
 [ 0.         0.         0.         0.         0.7125891 17.339668 ]
 [ 0.         0.         0.         0.         1.3233796 16.89854  ]]
Note: training and testing data is not equally distributed amongst classes, 
so it is normal that more than a 6th of the data is correctly classifier in the last category.
/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))