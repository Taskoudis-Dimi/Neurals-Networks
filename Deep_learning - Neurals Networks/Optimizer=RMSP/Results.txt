Training iter #1500:   Batch Loss = 2.981847, Accuracy = 0.15733332931995392
PERFORMANCE ON TEST SET: Batch Loss = 2.9204349517822266, Accuracy = 0.15914489328861237
Training iter #30000:   Batch Loss = 2.310324, Accuracy = 0.2199999988079071
PERFORMANCE ON TEST SET: Batch Loss = 2.3135664463043213, Accuracy = 0.25687140226364136
Training iter #60000:   Batch Loss = 1.996013, Accuracy = 0.4620000123977661
PERFORMANCE ON TEST SET: Batch Loss = 1.9889774322509766, Accuracy = 0.4672548472881317
Training iter #90000:   Batch Loss = 1.730274, Accuracy = 0.5566666722297668
PERFORMANCE ON TEST SET: Batch Loss = 1.7603542804718018, Accuracy = 0.5520868897438049
Training iter #120000:   Batch Loss = 1.394965, Accuracy = 0.6899999976158142
PERFORMANCE ON TEST SET: Batch Loss = 1.73628568649292, Accuracy = 0.5670173168182373
Training iter #150000:   Batch Loss = 1.307814, Accuracy = 0.7193333506584167
PERFORMANCE ON TEST SET: Batch Loss = 1.5268235206604004, Accuracy = 0.6359009146690369
Training iter #180000:   Batch Loss = 1.387811, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 1.3471888303756714, Accuracy = 0.7241262197494507
Training iter #210000:   Batch Loss = 1.045952, Accuracy = 0.8579999804496765
PERFORMANCE ON TEST SET: Batch Loss = 1.290321707725525, Accuracy = 0.7780793905258179
Training iter #240000:   Batch Loss = 0.840738, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 1.1278784275054932, Accuracy = 0.8150661587715149
Training iter #270000:   Batch Loss = 0.922292, Accuracy = 0.8786666393280029
PERFORMANCE ON TEST SET: Batch Loss = 1.090914011001587, Accuracy = 0.8187987804412842
Training iter #300000:   Batch Loss = 0.782076, Accuracy = 0.9293333292007446
PERFORMANCE ON TEST SET: Batch Loss = 1.0626999139785767, Accuracy = 0.8367831707000732
Training iter #330000:   Batch Loss = 0.751804, Accuracy = 0.95333331823349
PERFORMANCE ON TEST SET: Batch Loss = 0.9991261959075928, Accuracy = 0.8561248779296875
Training iter #360000:   Batch Loss = 0.716803, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.921677827835083, Accuracy = 0.8652867078781128
Training iter #390000:   Batch Loss = 1.013427, Accuracy = 0.8320000171661377
PERFORMANCE ON TEST SET: Batch Loss = 1.799622654914856, Accuracy = 0.6549032926559448
Training iter #420000:   Batch Loss = 0.641519, Accuracy = 0.9520000219345093
PERFORMANCE ON TEST SET: Batch Loss = 0.9448688626289368, Accuracy = 0.8517135977745056
Training iter #450000:   Batch Loss = 0.659962, Accuracy = 0.9179999828338623
PERFORMANCE ON TEST SET: Batch Loss = 2.15670108795166, Accuracy = 0.6593145728111267
Training iter #480000:   Batch Loss = 0.622665, Accuracy = 0.9380000233650208
PERFORMANCE ON TEST SET: Batch Loss = 0.8339763879776001, Accuracy = 0.8785205483436584
Training iter #510000:   Batch Loss = 0.640874, Accuracy = 0.940666675567627
PERFORMANCE ON TEST SET: Batch Loss = 0.9189505577087402, Accuracy = 0.8574821949005127
Training iter #540000:   Batch Loss = 0.646283, Accuracy = 0.9139999747276306
PERFORMANCE ON TEST SET: Batch Loss = 0.8487745523452759, Accuracy = 0.8741092681884766
Training iter #570000:   Batch Loss = 0.633932, Accuracy = 0.9259999990463257
PERFORMANCE ON TEST SET: Batch Loss = 0.8253973126411438, Accuracy = 0.8781812191009521
Training iter #600000:   Batch Loss = 0.606837, Accuracy = 0.9319999814033508
PERFORMANCE ON TEST SET: Batch Loss = 0.8257132172584534, Accuracy = 0.8683406710624695
Training iter #630000:   Batch Loss = 0.533104, Accuracy = 0.9679999947547913
PERFORMANCE ON TEST SET: Batch Loss = 0.7938395738601685, Accuracy = 0.8788598775863647
Training iter #660000:   Batch Loss = 0.513786, Accuracy = 0.9586666822433472
PERFORMANCE ON TEST SET: Batch Loss = 0.794697642326355, Accuracy = 0.8870037198066711
Training iter #690000:   Batch Loss = 0.482757, Accuracy = 0.9546666741371155
PERFORMANCE ON TEST SET: Batch Loss = 0.8017568588256836, Accuracy = 0.879199206829071
Training iter #720000:   Batch Loss = 0.545639, Accuracy = 0.9459999799728394
PERFORMANCE ON TEST SET: Batch Loss = 0.8007199764251709, Accuracy = 0.8656260371208191
Training iter #750000:   Batch Loss = 0.531267, Accuracy = 0.9419999718666077
PERFORMANCE ON TEST SET: Batch Loss = 0.769164502620697, Accuracy = 0.8819137811660767
Training iter #780000:   Batch Loss = 0.458271, Accuracy = 0.9660000205039978
PERFORMANCE ON TEST SET: Batch Loss = 0.7043676376342773, Accuracy = 0.8778418898582458
Training iter #810000:   Batch Loss = 0.465884, Accuracy = 0.9493333101272583
PERFORMANCE ON TEST SET: Batch Loss = 0.7750178575515747, Accuracy = 0.8951476216316223
Training iter #840000:   Batch Loss = 0.502350, Accuracy = 0.9313333630561829
PERFORMANCE ON TEST SET: Batch Loss = 0.7289789915084839, Accuracy = 0.8656260371208191
Training iter #870000:   Batch Loss = 0.445543, Accuracy = 0.9486666917800903
PERFORMANCE ON TEST SET: Batch Loss = 0.7630199193954468, Accuracy = 0.8785205483436584
Training iter #900000:   Batch Loss = 0.430932, Accuracy = 0.972000002861023
PERFORMANCE ON TEST SET: Batch Loss = 0.7517974376678467, Accuracy = 0.8720732927322388
Training iter #930000:   Batch Loss = 0.471906, Accuracy = 0.9259999990463257
PERFORMANCE ON TEST SET: Batch Loss = 0.785703182220459, Accuracy = 0.8605361580848694
Training iter #960000:   Batch Loss = 0.854519, Accuracy = 0.8486666679382324
PERFORMANCE ON TEST SET: Batch Loss = 1.184524416923523, Accuracy = 0.7560230493545532
Training iter #990000:   Batch Loss = 0.371124, Accuracy = 0.9806666374206543
PERFORMANCE ON TEST SET: Batch Loss = 0.7324265241622925, Accuracy = 0.8873430490493774
Training iter #1020000:   Batch Loss = 0.412112, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 0.7533795237541199, Accuracy = 0.8737699389457703
Training iter #1050000:   Batch Loss = 0.388777, Accuracy = 0.9346666932106018
PERFORMANCE ON TEST SET: Batch Loss = 0.674899697303772, Accuracy = 0.8836104273796082
Training iter #1080000:   Batch Loss = 0.382878, Accuracy = 0.9739999771118164
PERFORMANCE ON TEST SET: Batch Loss = 0.6997504830360413, Accuracy = 0.8907363414764404
Training iter #1110000:   Batch Loss = 0.439087, Accuracy = 0.9433333277702332
PERFORMANCE ON TEST SET: Batch Loss = 0.6906845569610596, Accuracy = 0.8839497566223145
Training iter #1140000:   Batch Loss = 0.405388, Accuracy = 0.9433333277702332
PERFORMANCE ON TEST SET: Batch Loss = 0.609383225440979, Accuracy = 0.8910756707191467
Training iter #1170000:   Batch Loss = 0.340952, Accuracy = 0.9613333344459534
PERFORMANCE ON TEST SET: Batch Loss = 0.7018460035324097, Accuracy = 0.8832710981369019
Training iter #1200000:   Batch Loss = 0.354281, Accuracy = 0.9526666402816772
PERFORMANCE ON TEST SET: Batch Loss = 0.6130316257476807, Accuracy = 0.8890396952629089
Training iter #1230000:   Batch Loss = 0.346580, Accuracy = 0.9553333520889282
PERFORMANCE ON TEST SET: Batch Loss = 0.6009945273399353, Accuracy = 0.894808292388916
Training iter #1260000:   Batch Loss = 0.336186, Accuracy = 0.9559999704360962
PERFORMANCE ON TEST SET: Batch Loss = 0.6117532849311829, Accuracy = 0.8900576829910278
Training iter #1290000:   Batch Loss = 0.424653, Accuracy = 0.8853333592414856
PERFORMANCE ON TEST SET: Batch Loss = 0.6932744979858398, Accuracy = 0.8866643905639648
Training iter #1320000:   Batch Loss = 0.401654, Accuracy = 0.9253333210945129
PERFORMANCE ON TEST SET: Batch Loss = 0.9039733409881592, Accuracy = 0.8262640237808228
Training iter #1350000:   Batch Loss = 0.309935, Accuracy = 0.9753333330154419
PERFORMANCE ON TEST SET: Batch Loss = 0.6291549205780029, Accuracy = 0.8866643905639648
Training iter #1380000:   Batch Loss = 0.332454, Accuracy = 0.9433333277702332
PERFORMANCE ON TEST SET: Batch Loss = 0.6250232458114624, Accuracy = 0.88802170753479
Training iter #1410000:   Batch Loss = 0.305905, Accuracy = 0.9446666836738586
PERFORMANCE ON TEST SET: Batch Loss = 0.534441351890564, Accuracy = 0.8971835970878601
Training iter #1440000:   Batch Loss = 0.312070, Accuracy = 0.9733333587646484
PERFORMANCE ON TEST SET: Batch Loss = 0.572384238243103, Accuracy = 0.9022734761238098
Training iter #1470000:   Batch Loss = 0.344735, Accuracy = 0.95333331823349
PERFORMANCE ON TEST SET: Batch Loss = 0.5930759906768799, Accuracy = 0.8961656093597412
Training iter #1500000:   Batch Loss = 0.383385, Accuracy = 0.9559999704360962
PERFORMANCE ON TEST SET: Batch Loss = 0.6221888065338135, Accuracy = 0.8839497566223145
Training iter #1530000:   Batch Loss = 0.278242, Accuracy = 0.9646666646003723
PERFORMANCE ON TEST SET: Batch Loss = 0.5867543816566467, Accuracy = 0.88802170753479
Training iter #1560000:   Batch Loss = 0.342444, Accuracy = 0.9346666932106018
PERFORMANCE ON TEST SET: Batch Loss = 0.5688837766647339, Accuracy = 0.8917543292045593
Training iter #1590000:   Batch Loss = 0.293348, Accuracy = 0.9453333616256714
PERFORMANCE ON TEST SET: Batch Loss = 0.542043149471283, Accuracy = 0.9019341468811035
Training iter #1620000:   Batch Loss = 0.276208, Accuracy = 0.9586666822433472
PERFORMANCE ON TEST SET: Batch Loss = 0.5703566670417786, Accuracy = 0.8975229263305664
Training iter #1650000:   Batch Loss = 0.310267, Accuracy = 0.9240000247955322
PERFORMANCE ON TEST SET: Batch Loss = 0.5207878947257996, Accuracy = 0.907024085521698
Training iter #1680000:   Batch Loss = 0.317245, Accuracy = 0.9293333292007446
PERFORMANCE ON TEST SET: Batch Loss = 0.5496447086334229, Accuracy = 0.8958262801170349
Training iter #1710000:   Batch Loss = 0.327436, Accuracy = 0.9473333358764648
PERFORMANCE ON TEST SET: Batch Loss = 0.5359436273574829, Accuracy = 0.9043094515800476
Training iter #1740000:   Batch Loss = 0.254157, Accuracy = 0.9773333072662354
PERFORMANCE ON TEST SET: Batch Loss = 0.5218691229820251, Accuracy = 0.8944689631462097
Training iter #1770000:   Batch Loss = 0.261169, Accuracy = 0.9506666660308838
PERFORMANCE ON TEST SET: Batch Loss = 0.5353337526321411, Accuracy = 0.9073634147644043
Training iter #1800000:   Batch Loss = 0.255501, Accuracy = 0.9453333616256714
PERFORMANCE ON TEST SET: Batch Loss = 0.5695226788520813, Accuracy = 0.8883610367774963
Training iter #1830000:   Batch Loss = 0.289848, Accuracy = 0.95333331823349
PERFORMANCE ON TEST SET: Batch Loss = 0.5004909038543701, Accuracy = 0.8944689631462097
Training iter #1860000:   Batch Loss = 0.317185, Accuracy = 0.937333345413208
PERFORMANCE ON TEST SET: Batch Loss = 0.5480402708053589, Accuracy = 0.894808292388916
Training iter #1890000:   Batch Loss = 0.269531, Accuracy = 0.9566666483879089
PERFORMANCE ON TEST SET: Batch Loss = 0.5187381505966187, Accuracy = 0.8998982310295105
Training iter #1920000:   Batch Loss = 0.246982, Accuracy = 0.9606666564941406
PERFORMANCE ON TEST SET: Batch Loss = 0.5278981924057007, Accuracy = 0.8992195725440979
Training iter #1950000:   Batch Loss = 0.267353, Accuracy = 0.9559999704360962
PERFORMANCE ON TEST SET: Batch Loss = 0.49060478806495667, Accuracy = 0.8965049386024475
Training iter #1980000:   Batch Loss = 0.258856, Accuracy = 0.9520000219345093
PERFORMANCE ON TEST SET: Batch Loss = 0.5019110441207886, Accuracy = 0.907024085521698
Training iter #2010000:   Batch Loss = 1.073510, Accuracy = 0.7933333516120911
PERFORMANCE ON TEST SET: Batch Loss = 0.530387818813324, Accuracy = 0.8758059144020081
Training iter #2040000:   Batch Loss = 0.311559, Accuracy = 0.9233333468437195
PERFORMANCE ON TEST SET: Batch Loss = 0.4952399730682373, Accuracy = 0.8802171945571899
Training iter #2070000:   Batch Loss = 0.590421, Accuracy = 0.8659999966621399
PERFORMANCE ON TEST SET: Batch Loss = 0.8109483122825623, Accuracy = 0.8072616457939148
Training iter #2100000:   Batch Loss = 0.220317, Accuracy = 0.9879999756813049
PERFORMANCE ON TEST SET: Batch Loss = 0.5268718004226685, Accuracy = 0.8900576829910278
Training iter #2130000:   Batch Loss = 0.238467, Accuracy = 0.9633333086967468
PERFORMANCE ON TEST SET: Batch Loss = 0.5635875463485718, Accuracy = 0.88802170753479
Training iter #2160000:   Batch Loss = 0.230118, Accuracy = 0.9613333344459534
PERFORMANCE ON TEST SET: Batch Loss = 0.5318780541419983, Accuracy = 0.8978622555732727
Training iter #2190000:   Batch Loss = 0.310718, Accuracy = 0.9520000219345093
PERFORMANCE ON TEST SET: Batch Loss = 0.5030080080032349, Accuracy = 0.891414999961853
Optimization Finished!
FINAL RESULT: Batch Loss = 0.46448245644569397, Accuracy = 0.9077027440071106



Testing Accuracy: 90.77027440071106%

Precision: 90.86721563021256%
Recall: 90.77027485578554%
f1_score: 90.72295408589997%

Confusion Matrix:
[[484   8   4   0   0   0]
 [ 28 430  13   0   0   0]
 [ 10   2 408   0   0   0]
 [  0  24   0 414  53   0]
 [  6   2   0 109 415   0]
 [  0   7   6   0   0 524]]

Confusion matrix (normalised to % of total test data):
[[16.423481    0.2714625   0.13573125  0.          0.          0.        ]
 [ 0.9501188  14.59111     0.44112659  0.          0.          0.        ]
 [ 0.3393281   0.06786563 13.844588    0.          0.          0.        ]
 [ 0.          0.8143875   0.         14.048184    1.798439    0.        ]
 [ 0.20359688  0.06786563  0.          3.6986763  14.082117    0.        ]
 [ 0.          0.2375297   0.20359688  0.          0.         17.780794  ]]
Note: training and testing data is not equally distributed amongst classes, 
so it is normal that more than a 6th of the data is correctly classifier in the last category.