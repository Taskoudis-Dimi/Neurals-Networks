Training iter #1500:   Batch Loss = 3.160554, Accuracy = 0.1926666647195816
PERFORMANCE ON TEST SET: Batch Loss = 2.8650875091552734, Accuracy = 0.1448931097984314
Training iter #30000:   Batch Loss = 1.810905, Accuracy = 0.5180000066757202
PERFORMANCE ON TEST SET: Batch Loss = 1.8557276725769043, Accuracy = 0.4882931709289551
Training iter #60000:   Batch Loss = 1.560214, Accuracy = 0.5933333039283752
PERFORMANCE ON TEST SET: Batch Loss = 1.637038230895996, Accuracy = 0.5609093904495239
Training iter #90000:   Batch Loss = 1.424073, Accuracy = 0.5866666436195374
PERFORMANCE ON TEST SET: Batch Loss = 1.4487193822860718, Accuracy = 0.6437054872512817
Training iter #120000:   Batch Loss = 1.186614, Accuracy = 0.75
PERFORMANCE ON TEST SET: Batch Loss = 1.3232824802398682, Accuracy = 0.7081778049468994
Training iter #150000:   Batch Loss = 1.481070, Accuracy = 0.5893333554267883
PERFORMANCE ON TEST SET: Batch Loss = 1.5214810371398926, Accuracy = 0.6043434143066406
Training iter #180000:   Batch Loss = 1.293276, Accuracy = 0.6859999895095825
PERFORMANCE ON TEST SET: Batch Loss = 1.3314104080200195, Accuracy = 0.6901934146881104
Training iter #210000:   Batch Loss = 1.220882, Accuracy = 0.6826666593551636
PERFORMANCE ON TEST SET: Batch Loss = 1.2793564796447754, Accuracy = 0.6983373165130615
Training iter #240000:   Batch Loss = 1.016772, Accuracy = 0.7913333177566528
PERFORMANCE ON TEST SET: Batch Loss = 1.2380127906799316, Accuracy = 0.7275195121765137
Training iter #270000:   Batch Loss = 0.941192, Accuracy = 0.8073333501815796
PERFORMANCE ON TEST SET: Batch Loss = 1.2650322914123535, Accuracy = 0.7064811587333679
Training iter #300000:   Batch Loss = 0.878762, Accuracy = 0.8740000128746033
PERFORMANCE ON TEST SET: Batch Loss = 1.1982040405273438, Accuracy = 0.7427892684936523
Training iter #330000:   Batch Loss = 0.864916, Accuracy = 0.871999979019165
PERFORMANCE ON TEST SET: Batch Loss = 1.2188258171081543, Accuracy = 0.7567017078399658
Training iter #360000:   Batch Loss = 0.889828, Accuracy = 0.8573333621025085
PERFORMANCE ON TEST SET: Batch Loss = 1.1457297801971436, Accuracy = 0.7899559140205383
Training iter #390000:   Batch Loss = 0.911469, Accuracy = 0.8326666951179504
PERFORMANCE ON TEST SET: Batch Loss = 1.2015433311462402, Accuracy = 0.7855446338653564
Training iter #420000:   Batch Loss = 0.834772, Accuracy = 0.871999979019165
PERFORMANCE ON TEST SET: Batch Loss = 1.0708805322647095, Accuracy = 0.8201560974121094
Training iter #450000:   Batch Loss = 1.032240, Accuracy = 0.7926666736602783
PERFORMANCE ON TEST SET: Batch Loss = 1.2388112545013428, Accuracy = 0.7678995728492737
Training iter #480000:   Batch Loss = 0.796104, Accuracy = 0.906000018119812
PERFORMANCE ON TEST SET: Batch Loss = 1.0141136646270752, Accuracy = 0.8167628049850464
Training iter #510000:   Batch Loss = 0.678812, Accuracy = 0.9506666660308838
PERFORMANCE ON TEST SET: Batch Loss = 1.0282453298568726, Accuracy = 0.8401764631271362
Training iter #540000:   Batch Loss = 0.760775, Accuracy = 0.8820000290870667
PERFORMANCE ON TEST SET: Batch Loss = 0.9757839441299438, Accuracy = 0.8506956100463867
Training iter #570000:   Batch Loss = 0.742170, Accuracy = 0.8813333511352539
PERFORMANCE ON TEST SET: Batch Loss = 0.9376027584075928, Accuracy = 0.8639293909072876
Training iter #600000:   Batch Loss = 0.708690, Accuracy = 0.8833333253860474
PERFORMANCE ON TEST SET: Batch Loss = 0.9179188013076782, Accuracy = 0.8727519512176514
Training iter #630000:   Batch Loss = 0.544277, Accuracy = 0.9513333439826965
PERFORMANCE ON TEST SET: Batch Loss = 0.8725446462631226, Accuracy = 0.884628415107727
Training iter #660000:   Batch Loss = 0.561537, Accuracy = 0.9673333168029785
PERFORMANCE ON TEST SET: Batch Loss = 0.8588152527809143, Accuracy = 0.879199206829071
Training iter #690000:   Batch Loss = 0.646700, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.8686332106590271, Accuracy = 0.8693586587905884
Training iter #720000:   Batch Loss = 0.629434, Accuracy = 0.9206666946411133
PERFORMANCE ON TEST SET: Batch Loss = 1.051108479499817, Accuracy = 0.8269426822662354
Training iter #750000:   Batch Loss = 0.686193, Accuracy = 0.890666663646698
PERFORMANCE ON TEST SET: Batch Loss = 0.8314146995544434, Accuracy = 0.8778418898582458
Training iter #780000:   Batch Loss = 0.521308, Accuracy = 0.9586666822433472
PERFORMANCE ON TEST SET: Batch Loss = 0.8060325384140015, Accuracy = 0.885646402835846
Training iter #810000:   Batch Loss = 0.527609, Accuracy = 0.9419999718666077
PERFORMANCE ON TEST SET: Batch Loss = 0.8507715463638306, Accuracy = 0.8805565237998962
Training iter #840000:   Batch Loss = 0.522579, Accuracy = 0.9433333277702332
PERFORMANCE ON TEST SET: Batch Loss = 0.8242658376693726, Accuracy = 0.8873430490493774
Training iter #870000:   Batch Loss = 0.492655, Accuracy = 0.9760000109672546
PERFORMANCE ON TEST SET: Batch Loss = 0.8256630301475525, Accuracy = 0.884628415107727
Training iter #900000:   Batch Loss = 0.475760, Accuracy = 0.9733333587646484
PERFORMANCE ON TEST SET: Batch Loss = 0.8313402533531189, Accuracy = 0.8764845728874207
Training iter #930000:   Batch Loss = 0.594602, Accuracy = 0.8960000276565552
PERFORMANCE ON TEST SET: Batch Loss = 0.8084895610809326, Accuracy = 0.8842890858650208
Training iter #960000:   Batch Loss = 0.730822, Accuracy = 0.8433333039283752
PERFORMANCE ON TEST SET: Batch Loss = 1.03068208694458, Accuracy = 0.8303359150886536
Training iter #990000:   Batch Loss = 0.477926, Accuracy = 0.9606666564941406
PERFORMANCE ON TEST SET: Batch Loss = 0.8124483823776245, Accuracy = 0.8795385360717773
Training iter #1020000:   Batch Loss = 0.465927, Accuracy = 0.9739999771118164
PERFORMANCE ON TEST SET: Batch Loss = 0.7655460834503174, Accuracy = 0.8805565237998962
Training iter #1050000:   Batch Loss = 0.438050, Accuracy = 0.984000027179718
PERFORMANCE ON TEST SET: Batch Loss = 0.7401604056358337, Accuracy = 0.8934509754180908
Training iter #1080000:   Batch Loss = 0.472429, Accuracy = 0.9726666808128357
PERFORMANCE ON TEST SET: Batch Loss = 0.7355806231498718, Accuracy = 0.891414999961853
Training iter #1110000:   Batch Loss = 0.504627, Accuracy = 0.9293333292007446
PERFORMANCE ON TEST SET: Batch Loss = 0.7768771648406982, Accuracy = 0.8900576829910278
Training iter #1140000:   Batch Loss = 0.489885, Accuracy = 0.9386666417121887
PERFORMANCE ON TEST SET: Batch Loss = 0.7481352090835571, Accuracy = 0.8887003660202026
Training iter #1170000:   Batch Loss = 0.427725, Accuracy = 0.9526666402816772
PERFORMANCE ON TEST SET: Batch Loss = 0.7439965605735779, Accuracy = 0.8900576829910278
Training iter #1200000:   Batch Loss = 0.421291, Accuracy = 0.9566666483879089
PERFORMANCE ON TEST SET: Batch Loss = 0.8061921000480652, Accuracy = 0.8859857320785522
Training iter #1230000:   Batch Loss = 0.417276, Accuracy = 0.9573333263397217
PERFORMANCE ON TEST SET: Batch Loss = 0.7440222501754761, Accuracy = 0.8887003660202026
Training iter #1260000:   Batch Loss = 0.384588, Accuracy = 0.9819999933242798
PERFORMANCE ON TEST SET: Batch Loss = 0.7347984313964844, Accuracy = 0.8931116461753845
Training iter #1290000:   Batch Loss = 0.499590, Accuracy = 0.9013333320617676
PERFORMANCE ON TEST SET: Batch Loss = 0.7620273232460022, Accuracy = 0.84764164686203
Training iter #1320000:   Batch Loss = 0.496754, Accuracy = 0.921999990940094
PERFORMANCE ON TEST SET: Batch Loss = 0.7231554388999939, Accuracy = 0.8713946342468262
Training iter #1350000:   Batch Loss = 0.614896, Accuracy = 0.8926666378974915
PERFORMANCE ON TEST SET: Batch Loss = 0.8703116774559021, Accuracy = 0.8411944508552551
Training iter #1380000:   Batch Loss = 0.452292, Accuracy = 0.9693333506584167
PERFORMANCE ON TEST SET: Batch Loss = 0.6604036092758179, Accuracy = 0.8870037198066711
Training iter #1410000:   Batch Loss = 0.371131, Accuracy = 0.984666645526886
PERFORMANCE ON TEST SET: Batch Loss = 0.6683688163757324, Accuracy = 0.8917543292045593
Training iter #1440000:   Batch Loss = 0.390427, Accuracy = 0.9819999933242798
PERFORMANCE ON TEST SET: Batch Loss = 0.6732649803161621, Accuracy = 0.8903970122337341
Training iter #1470000:   Batch Loss = 0.441220, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 0.6782495975494385, Accuracy = 0.8761452436447144
Training iter #1500000:   Batch Loss = 0.455851, Accuracy = 0.9346666932106018
PERFORMANCE ON TEST SET: Batch Loss = 0.6217325925827026, Accuracy = 0.8873430490493774
Training iter #1530000:   Batch Loss = 0.384252, Accuracy = 0.9539999961853027
PERFORMANCE ON TEST SET: Batch Loss = 0.6776584386825562, Accuracy = 0.8927723169326782
Training iter #1560000:   Batch Loss = 0.366249, Accuracy = 0.9580000042915344
PERFORMANCE ON TEST SET: Batch Loss = 0.6687593460083008, Accuracy = 0.894808292388916
Training iter #1590000:   Batch Loss = 0.381165, Accuracy = 0.9473333358764648
PERFORMANCE ON TEST SET: Batch Loss = 0.6841221451759338, Accuracy = 0.8995589017868042
Training iter #1620000:   Batch Loss = 0.364330, Accuracy = 0.9620000123977661
PERFORMANCE ON TEST SET: Batch Loss = 0.6924407482147217, Accuracy = 0.8968442678451538
Training iter #1650000:   Batch Loss = 0.442821, Accuracy = 0.9153333306312561
PERFORMANCE ON TEST SET: Batch Loss = 0.6778757572174072, Accuracy = 0.8954869508743286
Training iter #1680000:   Batch Loss = 0.429126, Accuracy = 0.9233333468437195
PERFORMANCE ON TEST SET: Batch Loss = 0.6732689142227173, Accuracy = 0.8971835970878601
Training iter #1710000:   Batch Loss = 0.446736, Accuracy = 0.9213333129882812
PERFORMANCE ON TEST SET: Batch Loss = 0.6467732191085815, Accuracy = 0.9015948176383972
Training iter #1740000:   Batch Loss = 0.354223, Accuracy = 0.9646666646003723
PERFORMANCE ON TEST SET: Batch Loss = 0.6850286722183228, Accuracy = 0.8870037198066711
Training iter #1770000:   Batch Loss = 0.331429, Accuracy = 0.9666666388511658
PERFORMANCE ON TEST SET: Batch Loss = 0.6766065955162048, Accuracy = 0.891414999961853
Training iter #1800000:   Batch Loss = 0.313632, Accuracy = 0.9779999852180481
PERFORMANCE ON TEST SET: Batch Loss = 0.6339024901390076, Accuracy = 0.9039701223373413
Training iter #1830000:   Batch Loss = 0.374181, Accuracy = 0.9539999961853027
PERFORMANCE ON TEST SET: Batch Loss = 0.6463382244110107, Accuracy = 0.8941296339035034
Training iter #1860000:   Batch Loss = 0.382878, Accuracy = 0.9380000233650208
PERFORMANCE ON TEST SET: Batch Loss = 0.6774523258209229, Accuracy = 0.8920936584472656
Training iter #1890000:   Batch Loss = 0.787923, Accuracy = 0.8053333163261414
PERFORMANCE ON TEST SET: Batch Loss = 0.9803047776222229, Accuracy = 0.7343060970306396
Training iter #1920000:   Batch Loss = 0.542389, Accuracy = 0.8566666841506958
PERFORMANCE ON TEST SET: Batch Loss = 0.8107873201370239, Accuracy = 0.8177807927131653
Training iter #1950000:   Batch Loss = 0.365195, Accuracy = 0.9526666402816772
PERFORMANCE ON TEST SET: Batch Loss = 0.5403577089309692, Accuracy = 0.8941296339035034
Training iter #1980000:   Batch Loss = 0.367940, Accuracy = 0.9413333535194397
PERFORMANCE ON TEST SET: Batch Loss = 0.5614462494850159, Accuracy = 0.8903970122337341
Training iter #2010000:   Batch Loss = 0.298821, Accuracy = 0.9826666712760925
PERFORMANCE ON TEST SET: Batch Loss = 0.5793567299842834, Accuracy = 0.8890396952629089
Training iter #2040000:   Batch Loss = 0.400210, Accuracy = 0.918666660785675
PERFORMANCE ON TEST SET: Batch Loss = 0.581081748008728, Accuracy = 0.891414999961853
Training iter #2070000:   Batch Loss = 0.388048, Accuracy = 0.9380000233650208
PERFORMANCE ON TEST SET: Batch Loss = 0.5423640012741089, Accuracy = 0.900237500667572
Training iter #2100000:   Batch Loss = 0.300432, Accuracy = 0.9879999756813049
PERFORMANCE ON TEST SET: Batch Loss = 0.551344096660614, Accuracy = 0.8954869508743286
Training iter #2130000:   Batch Loss = 0.293309, Accuracy = 0.9833333492279053
PERFORMANCE ON TEST SET: Batch Loss = 0.5492069721221924, Accuracy = 0.898201584815979
Training iter #2160000:   Batch Loss = 0.282594, Accuracy = 0.9866666793823242
PERFORMANCE ON TEST SET: Batch Loss = 0.5414912104606628, Accuracy = 0.8988802433013916
Training iter #2190000:   Batch Loss = 0.297414, Accuracy = 0.9819999933242798
PERFORMANCE ON TEST SET: Batch Loss = 0.5848972797393799, Accuracy = 0.8761452436447144
Optimization Finished!
FINAL RESULT: Batch Loss = 0.6021398305892944, Accuracy = 0.88802170753479


Testing Accuracy: 88.802170753479%

Precision: 89.65626380401187%
Recall: 88.80217170003394%
f1_score: 88.87955854192894%

Confusion Matrix:
[[476   1   2  14   3   0]
 [ 29 407  15  18   0   2]
 [  7  27 385   1   0   0]
 [  1   6   0 441  43   0]
 [  0   0   0 145 387   0]
 [  0  16   0   0   0 521]]

Confusion matrix (normalised to % of total test data):
[[16.152018    0.03393281  0.06786563  0.4750594   0.10179844  0.        ]
 [ 0.9840515  13.810656    0.5089922   0.6107906   0.          0.06786563]
 [ 0.2375297   0.916186   13.064133    0.03393281  0.          0.        ]
 [ 0.03393281  0.20359688  0.         14.964371    1.459111    0.        ]
 [ 0.          0.          0.          4.920258   13.131998    0.        ]
 [ 0.          0.542925    0.          0.          0.         17.678995  ]]
Note: training and testing data is not equally distributed amongst classes, 
so it is normal that more than a 6th of the data is correctly classifier in the last category.